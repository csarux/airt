<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Guion de presentación: Inteligencia Artificial en Radioterapia</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="guion_completo_ia_radioterapia_files/libs/clipboard/clipboard.min.js"></script>
<script src="guion_completo_ia_radioterapia_files/libs/quarto-html/quarto.js"></script>
<script src="guion_completo_ia_radioterapia_files/libs/quarto-html/popper.min.js"></script>
<script src="guion_completo_ia_radioterapia_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="guion_completo_ia_radioterapia_files/libs/quarto-html/anchor.min.js"></script>
<link href="guion_completo_ia_radioterapia_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="guion_completo_ia_radioterapia_files/libs/quarto-html/quarto-syntax-highlighting-e26003cea8cd680ca0c55a263523d882.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="guion_completo_ia_radioterapia_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="guion_completo_ia_radioterapia_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="guion_completo_ia_radioterapia_files/libs/bootstrap/bootstrap-48aa64f023705799beb441fb4133f72b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">


</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introducción" id="toc-introducción" class="nav-link active" data-scroll-target="#introducción"><span class="header-section-number">1</span> Introducción</a></li>
  <li><a href="#qué-es-la-inteligencia-artificial" id="toc-qué-es-la-inteligencia-artificial" class="nav-link" data-scroll-target="#qué-es-la-inteligencia-artificial"><span class="header-section-number">2</span> ¿Qué es la inteligencia artificial?</a></li>
  <li><a href="#definición-y-ejemplos-cotidianos-de-ia" id="toc-definición-y-ejemplos-cotidianos-de-ia" class="nav-link" data-scroll-target="#definición-y-ejemplos-cotidianos-de-ia"><span class="header-section-number">3</span> Definición y ejemplos cotidianos de IA</a></li>
  <li><a href="#en-qué-se-diferencia-de-un-software-tradicional" id="toc-en-qué-se-diferencia-de-un-software-tradicional" class="nav-link" data-scroll-target="#en-qué-se-diferencia-de-un-software-tradicional"><span class="header-section-number">4</span> ¿En qué se diferencia de un software tradicional?</a>
  <ul class="collapse">
  <li><a href="#comparativa" id="toc-comparativa" class="nav-link" data-scroll-target="#comparativa"><span class="header-section-number">4.1</span> Comparativa</a></li>
  <li><a href="#analogía-culinaria" id="toc-analogía-culinaria" class="nav-link" data-scroll-target="#analogía-culinaria"><span class="header-section-number">4.2</span> Analogía culinaria</a></li>
  </ul></li>
  <li><a href="#tipos-principales-de-ia" id="toc-tipos-principales-de-ia" class="nav-link" data-scroll-target="#tipos-principales-de-ia"><span class="header-section-number">5</span> Tipos principales de IA</a>
  <ul class="collapse">
  <li><a href="#ia-simbólica" id="toc-ia-simbólica" class="nav-link" data-scroll-target="#ia-simbólica"><span class="header-section-number">5.1</span> IA simbólica</a></li>
  <li><a href="#ia-conexionista-redes-neuronales" id="toc-ia-conexionista-redes-neuronales" class="nav-link" data-scroll-target="#ia-conexionista-redes-neuronales"><span class="header-section-number">5.2</span> IA conexionista (redes neuronales)</a></li>
  </ul></li>
  <li><a href="#ejemplos" id="toc-ejemplos" class="nav-link" data-scroll-target="#ejemplos"><span class="header-section-number">6</span> Ejemplos</a></li>
  <li><a href="#software-tradicional-cálculo-de-dosis" id="toc-software-tradicional-cálculo-de-dosis" class="nav-link" data-scroll-target="#software-tradicional-cálculo-de-dosis"><span class="header-section-number">7</span> Software tradicional, cálculo de dosis</a></li>
  <li><a href="#ia-tradicional" id="toc-ia-tradicional" class="nav-link" data-scroll-target="#ia-tradicional"><span class="header-section-number">8</span> IA tradicional</a></li>
  <li><a href="#cómo-podemos-hacer-pensar-a-una-máquina" id="toc-cómo-podemos-hacer-pensar-a-una-máquina" class="nav-link" data-scroll-target="#cómo-podemos-hacer-pensar-a-una-máquina"><span class="header-section-number">9</span> ¿Cómo podemos hacer pensar a una máquina?</a></li>
  <li><a href="#problemas-de-minimización-fundamentos-matemáticos" id="toc-problemas-de-minimización-fundamentos-matemáticos" class="nav-link" data-scroll-target="#problemas-de-minimización-fundamentos-matemáticos"><span class="header-section-number">10</span> Problemas de minimización, fundamentos matemáticos</a></li>
  <li><a href="#problemas-de-minimización-elementos-clave" id="toc-problemas-de-minimización-elementos-clave" class="nav-link" data-scroll-target="#problemas-de-minimización-elementos-clave"><span class="header-section-number">11</span> Problemas de minimización, elementos clave</a></li>
  <li><a href="#varias-variables-con-diferentes-correlaciones" id="toc-varias-variables-con-diferentes-correlaciones" class="nav-link" data-scroll-target="#varias-variables-con-diferentes-correlaciones"><span class="header-section-number">12</span> Varias variables con diferentes correlaciones</a></li>
  <li><a href="#overfitting-sobreajuste" id="toc-overfitting-sobreajuste" class="nav-link" data-scroll-target="#overfitting-sobreajuste"><span class="header-section-number">13</span> Overfitting (sobreajuste)</a></li>
  <li><a href="#ejemplos-de-minimización.-aplicación-a-la-radioterapia" id="toc-ejemplos-de-minimización.-aplicación-a-la-radioterapia" class="nav-link" data-scroll-target="#ejemplos-de-minimización.-aplicación-a-la-radioterapia"><span class="header-section-number">14</span> Ejemplos de minimización. Aplicación a la radioterapia</a></li>
  <li><a href="#optimación-de-fluencias-para-tratamientos-vmat" id="toc-optimación-de-fluencias-para-tratamientos-vmat" class="nav-link" data-scroll-target="#optimación-de-fluencias-para-tratamientos-vmat"><span class="header-section-number">15</span> Optimación de fluencias para tratamientos VMAT</a></li>
  <li><a href="#section" id="toc-section" class="nav-link" data-scroll-target="#section"><span class="header-section-number">16</span> </a></li>
  <li><a href="#correlaciones-entre-parámetros" id="toc-correlaciones-entre-parámetros" class="nav-link" data-scroll-target="#correlaciones-entre-parámetros"><span class="header-section-number">17</span> Correlaciones entre parámetros</a></li>
  <li><a href="#section-1" id="toc-section-1" class="nav-link" data-scroll-target="#section-1"><span class="header-section-number">18</span> </a></li>
  <li><a href="#cómo-funciona-una-red-neuronal-artificial" id="toc-cómo-funciona-una-red-neuronal-artificial" class="nav-link" data-scroll-target="#cómo-funciona-una-red-neuronal-artificial"><span class="header-section-number">19</span> ¿Cómo funciona una red neuronal artificial?</a></li>
  <li><a href="#section-2" id="toc-section-2" class="nav-link" data-scroll-target="#section-2"><span class="header-section-number">20</span> </a></li>
  <li><a href="#detalles-de-las-capas" id="toc-detalles-de-las-capas" class="nav-link" data-scroll-target="#detalles-de-las-capas"><span class="header-section-number">21</span> Detalles de las capas</a>
  <ul class="collapse">
  <li><a href="#capa-de-convolución-solo-para-redes-de-imagen" id="toc-capa-de-convolución-solo-para-redes-de-imagen" class="nav-link" data-scroll-target="#capa-de-convolución-solo-para-redes-de-imagen"><span class="header-section-number">21.1</span> Capa de convolución (solo para redes de imagen)</a></li>
  </ul></li>
  <li><a href="#visualización-de-la-salida-de-algunos-filtros-de-convolución" id="toc-visualización-de-la-salida-de-algunos-filtros-de-convolución" class="nav-link" data-scroll-target="#visualización-de-la-salida-de-algunos-filtros-de-convolución"><span class="header-section-number">22</span> Visualización de la salida de algunos filtros de convolución</a></li>
  <li><a href="#función-de-activación" id="toc-función-de-activación" class="nav-link" data-scroll-target="#función-de-activación"><span class="header-section-number">23</span> Función de activación</a></li>
  <li><a href="#pooling-reducción" id="toc-pooling-reducción" class="nav-link" data-scroll-target="#pooling-reducción"><span class="header-section-number">24</span> Pooling (reducción)</a></li>
  <li><a href="#cómo-aprende-una-red-neuronal" id="toc-cómo-aprende-una-red-neuronal" class="nav-link" data-scroll-target="#cómo-aprende-una-red-neuronal"><span class="header-section-number">25</span> ¿Cómo aprende una red neuronal?</a></li>
  <li><a href="#entrenamiento-minimizar-un-error" id="toc-entrenamiento-minimizar-un-error" class="nav-link" data-scroll-target="#entrenamiento-minimizar-un-error"><span class="header-section-number">26</span> Entrenamiento = Minimizar un error</a></li>
  <li><a href="#tipos-de-aprendizaje" id="toc-tipos-de-aprendizaje" class="nav-link" data-scroll-target="#tipos-de-aprendizaje"><span class="header-section-number">27</span> Tipos de aprendizaje</a></li>
  <li><a href="#aprendizaje-no-supervisado" id="toc-aprendizaje-no-supervisado" class="nav-link" data-scroll-target="#aprendizaje-no-supervisado"><span class="header-section-number">28</span> Aprendizaje No Supervisado</a></li>
  <li><a href="#aprendizaje-auto-supervisado" id="toc-aprendizaje-auto-supervisado" class="nav-link" data-scroll-target="#aprendizaje-auto-supervisado"><span class="header-section-number">29</span> Aprendizaje Auto-supervisado</a></li>
  <li><a href="#resumen-sobre-los-tipos-de-aprendizaje-en-ia" id="toc-resumen-sobre-los-tipos-de-aprendizaje-en-ia" class="nav-link" data-scroll-target="#resumen-sobre-los-tipos-de-aprendizaje-en-ia"><span class="header-section-number">30</span> Resumen sobre los tipos de aprendizaje en IA</a></li>
  <li><a href="#ejemplo-entrenamiento-de-una-red-neuronal-de-reconocimiento-facial" id="toc-ejemplo-entrenamiento-de-una-red-neuronal-de-reconocimiento-facial" class="nav-link" data-scroll-target="#ejemplo-entrenamiento-de-una-red-neuronal-de-reconocimiento-facial"><span class="header-section-number">31</span> Ejemplo: entrenamiento de una red neuronal de reconocimiento facial</a></li>
  <li><a href="#optimización-por-retropropagación" id="toc-optimización-por-retropropagación" class="nav-link" data-scroll-target="#optimización-por-retropropagación"><span class="header-section-number">32</span> Optimización por retropropagación</a></li>
  <li><a href="#modelos-de-inteligencia-artificial-clasificación-vs-regresión" id="toc-modelos-de-inteligencia-artificial-clasificación-vs-regresión" class="nav-link" data-scroll-target="#modelos-de-inteligencia-artificial-clasificación-vs-regresión"><span class="header-section-number">33</span> Modelos de Inteligencia Artificial: Clasificación vs Regresión</a></li>
  <li><a href="#naturaleza-del-problema" id="toc-naturaleza-del-problema" class="nav-link" data-scroll-target="#naturaleza-del-problema"><span class="header-section-number">34</span> Naturaleza del problema</a></li>
  <li><a href="#estructura-de-modelos-y-activaciones" id="toc-estructura-de-modelos-y-activaciones" class="nav-link" data-scroll-target="#estructura-de-modelos-y-activaciones"><span class="header-section-number">35</span> Estructura de modelos y activaciones</a></li>
  <li><a href="#aplicación-práctica-de-la-inteligencia-artificial-en-radioterapia" id="toc-aplicación-práctica-de-la-inteligencia-artificial-en-radioterapia" class="nav-link" data-scroll-target="#aplicación-práctica-de-la-inteligencia-artificial-en-radioterapia"><span class="header-section-number">36</span> Aplicación práctica de la inteligencia artificial en radioterapia</a></li>
  <li><a href="#campos-de-aplicación" id="toc-campos-de-aplicación" class="nav-link" data-scroll-target="#campos-de-aplicación"><span class="header-section-number">37</span> Campos de aplicación</a></li>
  <li><a href="#registro-deformable-de-imagen" id="toc-registro-deformable-de-imagen" class="nav-link" data-scroll-target="#registro-deformable-de-imagen"><span class="header-section-number">38</span> Registro deformable de imagen</a></li>
  <li><a href="#section-3" id="toc-section-3" class="nav-link" data-scroll-target="#section-3"><span class="header-section-number">39</span> </a></li>
  <li><a href="#section-4" id="toc-section-4" class="nav-link" data-scroll-target="#section-4"><span class="header-section-number">40</span> </a></li>
  <li><a href="#section-5" id="toc-section-5" class="nav-link" data-scroll-target="#section-5"><span class="header-section-number">41</span> </a></li>
  <li><a href="#métodos-de-realización-del-registro-deformable" id="toc-métodos-de-realización-del-registro-deformable" class="nav-link" data-scroll-target="#métodos-de-realización-del-registro-deformable"><span class="header-section-number">42</span> Métodos de realización del registro deformable</a></li>
  <li><a href="#métodos-basados-en-aprendizaje-profundo-deep-learning-dir" id="toc-métodos-basados-en-aprendizaje-profundo-deep-learning-dir" class="nav-link" data-scroll-target="#métodos-basados-en-aprendizaje-profundo-deep-learning-dir"><span class="header-section-number">43</span> Métodos basados en aprendizaje profundo (Deep Learning DIR)</a></li>
  <li><a href="#comparativa-de-modelos-de-deep-learning-para-dir" id="toc-comparativa-de-modelos-de-deep-learning-para-dir" class="nav-link" data-scroll-target="#comparativa-de-modelos-de-deep-learning-para-dir"><span class="header-section-number">44</span> Comparativa de modelos de Deep Learning para DIR</a></li>
  <li><a href="#section-6" id="toc-section-6" class="nav-link" data-scroll-target="#section-6"><span class="header-section-number">45</span> </a></li>
  <li><a href="#delimitación-de-volúmenes.-métodos-por-software-tradicional" id="toc-delimitación-de-volúmenes.-métodos-por-software-tradicional" class="nav-link" data-scroll-target="#delimitación-de-volúmenes.-métodos-por-software-tradicional"><span class="header-section-number">46</span> Delimitación de volúmenes. Métodos por software tradicional</a></li>
  <li><a href="#section-7" id="toc-section-7" class="nav-link" data-scroll-target="#section-7"><span class="header-section-number">47</span> </a></li>
  <li><a href="#section-8" id="toc-section-8" class="nav-link" data-scroll-target="#section-8"><span class="header-section-number">48</span> </a></li>
  <li><a href="#delimitación-de-volúmenes-mediante-atlas" id="toc-delimitación-de-volúmenes-mediante-atlas" class="nav-link" data-scroll-target="#delimitación-de-volúmenes-mediante-atlas"><span class="header-section-number">49</span> Delimitación de volúmenes mediante atlas</a></li>
  <li><a href="#section-9" id="toc-section-9" class="nav-link" data-scroll-target="#section-9"><span class="header-section-number">50</span> </a></li>
  <li><a href="#delimitación-de-volúmenes-mediante-ia-basada-en-cnns" id="toc-delimitación-de-volúmenes-mediante-ia-basada-en-cnns" class="nav-link" data-scroll-target="#delimitación-de-volúmenes-mediante-ia-basada-en-cnns"><span class="header-section-number">51</span> Delimitación de volúmenes mediante IA basada en CNNs</a></li>
  <li><a href="#arquitecturas-más-comunes" id="toc-arquitecturas-más-comunes" class="nav-link" data-scroll-target="#arquitecturas-más-comunes"><span class="header-section-number">52</span> Arquitecturas más comunes</a></li>
  <li><a href="#variantes-modernas-de-cnns-para-segmentación-médica" id="toc-variantes-modernas-de-cnns-para-segmentación-médica" class="nav-link" data-scroll-target="#variantes-modernas-de-cnns-para-segmentación-médica"><span class="header-section-number">53</span> Variantes modernas de CNNs para segmentación médica</a></li>
  <li><a href="#entrenamiento-de-modelos-de-segmentación-con-cnns" id="toc-entrenamiento-de-modelos-de-segmentación-con-cnns" class="nav-link" data-scroll-target="#entrenamiento-de-modelos-de-segmentación-con-cnns"><span class="header-section-number">54</span> Entrenamiento de modelos de segmentación con CNNs</a></li>
  <li><a href="#ejemplos-clínicos-reales-de-uso-de-cnns" id="toc-ejemplos-clínicos-reales-de-uso-de-cnns" class="nav-link" data-scroll-target="#ejemplos-clínicos-reales-de-uso-de-cnns"><span class="header-section-number">55</span> Ejemplos clínicos reales de uso de CNNs</a></li>
  <li><a href="#sistemas-comerciales-basados-en-cnn" id="toc-sistemas-comerciales-basados-en-cnn" class="nav-link" data-scroll-target="#sistemas-comerciales-basados-en-cnn"><span class="header-section-number">56</span> Sistemas comerciales basados en CNN</a></li>
  <li><a href="#generación-de-imágenes-sintéticas" id="toc-generación-de-imágenes-sintéticas" class="nav-link" data-scroll-target="#generación-de-imágenes-sintéticas"><span class="header-section-number">57</span> Generación de imágenes sintéticas</a></li>
  <li><a href="#predicción-de-dosis-y-planificación-automática" id="toc-predicción-de-dosis-y-planificación-automática" class="nav-link" data-scroll-target="#predicción-de-dosis-y-planificación-automática"><span class="header-section-number">58</span> Predicción de dosis y planificación automática</a></li>
  <li><a href="#validación-y-fiabilidad" id="toc-validación-y-fiabilidad" class="nav-link" data-scroll-target="#validación-y-fiabilidad"><span class="header-section-number">59</span> Validación y fiabilidad</a></li>
  <li><a href="#seguridad-y-responsabilidad" id="toc-seguridad-y-responsabilidad" class="nav-link" data-scroll-target="#seguridad-y-responsabilidad"><span class="header-section-number">60</span> Seguridad y responsabilidad</a></li>
  <li><a href="#el-papel-del-técnico-y-del-físico" id="toc-el-papel-del-técnico-y-del-físico" class="nav-link" data-scroll-target="#el-papel-del-técnico-y-del-físico"><span class="header-section-number">61</span> El papel del técnico y del físico</a></li>
  <li><a href="#conclusión" id="toc-conclusión" class="nav-link" data-scroll-target="#conclusión"><span class="header-section-number">62</span> Conclusión</a></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Guion de presentación: Inteligencia Artificial en Radioterapia</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introducción" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introducción</h1>
<p>Muy buenos días a todos y gracias por estar aquí.<br>
Quiero dar las gracias a los coodinadores de este Congreso por invitarme a este simposio. Es un placer poder compartir esta sesión con todos vosotros y con algunos compañeros físicos. Hoy os quiero hablar de inteligencia artificial, sí, ese tema del que todo el mundo habla… pero desde una perspectiva de aplicación a nuestro campo la radioterapia.</p>
<p>Y no, no os voy a hablar de ciencia ficción ni de robots que nos van a quitar el trabajo. Os voy a hablar de cómo la IA ya está ayudando —y puede ayudarnos aún más— en nuestro día a día en el hospital.</p>
</section>
<section id="qué-es-la-inteligencia-artificial" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> ¿Qué es la inteligencia artificial?</h1>
</section>
<section id="definición-y-ejemplos-cotidianos-de-ia" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Definición y ejemplos cotidianos de IA</h1>
<p>La inteligencia artificial es una rama de la informática que busca que las máquinas puedan imitar funciones humanas como aprender, razonar, resolver problemas o tomar decisiones. En otras palabras: que el ordenador no solo calcule lo que le decimos, sino que ‘aprenda’ a partir de ejemplos.</p>
<p>Y esto no es ciencia de laboratorio. La IA está en nuestra vida diaria, todo el rato. Por ejemplo:</p>
<ul>
<li>Cuando te llega un correo sospechoso y tu gestor de correo lo manda a la carpeta de spam.</li>
<li>Cuando Netflix te recomienda una serie sabiendo que probablemente te va a gustar.</li>
<li>Cuando usas un traductor automático, o hablas con un chatbot para preparar una charla como esta. Sí, sí esta charla ha sido preparada empleando inteligencia artificial.</li>
<li>Y en medicina, claro: por ejemplo, para detectar tumores en mamografías o en otras imágenes.</li>
</ul>
<p>Es decir, convivimos permanentemente con la inteligencia artificial y desde hace mucho tiempo, desde que estos sistemas estaban en mantillas, los medicos en general, de cualquier especialidad, siempre han mostrado gran interés por incorporar estas tecnologías en su práctica diaria. Ahora, con los grandes avanves que se han producido en este campo, probablemente asistimos, y somos protagonistas, de una revolución en nuestro modo de trabajar.</p>
</section>
<section id="en-qué-se-diferencia-de-un-software-tradicional" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> ¿En qué se diferencia de un software tradicional?</h1>
<p>Esto es importante. ¿En qué se diferencia un programa de IA de un software tradicional, como los que usamos para planificar un tratamiento o calcular una dosis?</p>
<p>El software tradicional funciona con instrucciones fijas. Tú le dices exactamente qué hacer: ‘suma esto’, ‘haz esta fórmula’, ‘calcula esa curva’.</p>
<p>En cambio, la IA no se basa en fórmulas predefinidas, sino que aprende a partir de ejemplos. Le damos datos, y a base de entrenarse con muchos casos, ‘aprende’ a resolver problemas nuevos.</p>
<section id="comparativa" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="comparativa"><span class="header-section-number">4.1</span> Comparativa</h2>
<p>Aquí tenéis una tabla resumen para verlo claro:</p>
<ul>
<li>El software tradicional está programado de forma explícita. La IA, en cambio, se entrena con datos.</li>
<li>El software clásico tiene una lógica fija. La IA es adaptable: si cambia el contexto, puede seguir funcionando bien.</li>
<li>En radioterapia, un ejemplo clásico de software sería el cálculo de dosis con fórmulas físicas.</li>
<li>Pero la IA puede hacer cosas como segmentar automáticamente una imagen médica.</li>
</ul>
</section>
<section id="analogía-culinaria" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="analogía-culinaria"><span class="header-section-number">4.2</span> Analogía culinaria</h2>
<p>Un software tradicional es como una receta de cocina: paso 1, paso 2, paso 3… Tú sigues las instrucciones, y si las sigues bien, te sale el plato.</p>
<p>La IA, en cambio, es como un cocinero que va aprendiendo a base de probar. La primera vez igual se le quema un poco. La segunda le queda mejor. Y al final acaba haciéndolo perfecto, incluso con ingredientes distintos.</p>
</section>
</section>
<section id="tipos-principales-de-ia" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Tipos principales de IA</h1>
<section id="ia-simbólica" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="ia-simbólica"><span class="header-section-number">5.1</span> IA simbólica</h2>
<p>Para hablar de los tipos principales de inteligencia artificial tenemos que empezar por la IA simbólica. Históricamente es la primera forma de inteligencia artificial que se planteó. Se basa en reglas lógicas que alguien ha programado. Por ejemplo: si ves esto, haz aquello. Son sistemas explicables, pero poco flexibles y que en general no funcionan muy bien.</p>
</section>
<section id="ia-conexionista-redes-neuronales" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="ia-conexionista-redes-neuronales"><span class="header-section-number">5.2</span> IA conexionista (redes neuronales)</h2>
<p>Otro tipo de inteligencia artificial es la denominada IA conexionista o basada en redes neuronales. Aquí ya hablamos de aprendizaje automático, de modelos que aprenden a partir de miles de ejemplos. Esto es lo que usamos, por ejemplo, en segmentación automática o en generación de imágenes sintéticas. Históricamente podemos distinguir dos subgrupos. El primero es el denominado Machine Learning. El segundo, el Deep learning. La distinción entre ellos se basa en cuanto de complejas sean las redes neuronales que emplean. Para simplificar podemos pensar que el Deep Learning es una evolución del Machine Learning que permite que el preprocesado de los datos para entrenar la red, o para ser analizados por la red, sea realizado por la propia red.</p>
</section>
</section>
<section id="ejemplos" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Ejemplos</h1>
<p>Veamos algunos ejemplos de lo explicado hasta ahora dentro de nuestra rutina habitual</p>
</section>
<section id="software-tradicional-cálculo-de-dosis" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> Software tradicional, cálculo de dosis</h1>
<p>Un ejemplo de software tradicional en nuestro campo son los algoritmos de cálculo de dosis. Esta ecuaciones que veis aquí son la ecuación de transporte Boltzmann para un campo de fotones. Lo de menos ahora es lo que significa, lo importante es ver que son suficiente complejas como para no intentar resolverlas a mano. De hecho, estas ecuaciones se han considerado inabordables numéricamente durante muchos años. Sin embargo, los últimos años ya tenemos algoritmos comerciales que las resuelven Y lo hacen de forma muy eficiente.</p>
<p>Este tipo de soluciones mediante software tradicional responden a este paradigma: sabemos cómo funcionan y cuando funcionan, lo hacen realmente bien, son muy eficientes. Pero solo funcionan para resolver un problema concreto, el problema para el que están programados y en las condiciones impuestas por su diseño. Una pequeña variación en los datos de entrada y el programa nos devuelve un error.</p>
</section>
<section id="ia-tradicional" class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span> IA tradicional</h1>
<p>Algunos problemas, sin embargo, se basan fundamentalmente en variaciones en la entrada. Por ejemplo, ocurre así con el conocimiento de caracteres o el reconocimiento facial. Los humanos destacamos por nuestra capacidad de abstracción. En esta tabla, por ejemplo, somos capaces de distinguir cada que aparece un 9, a pesar de que tenemos una gran variedad de formas en las que el 9 está escrito. Lo mismo nos pasa con las caras, somos capaces de reconocer a una persona, incluso si la vemos a mucha distancia o si está sonriendo, serio, llorando.</p>
<p>Si intentamos atacar este problema por la aproximación clásica, es decir, planteando una cierta lógica: si ves esto, se trata de esto, resulta que el sistema necesita muchos recursos, se vuelve tremendamente complejo y además los resultados que obtiene son bastante inferiores a los que obtiene un humano.</p>
<p>Así que este tipo de inteligencia artificial se encontraba bajo el paradigma: Sabemos cómo funciona, pero no funciona.</p>
<p>Para tener una mejor inteligencia artificial necesitamos desarrollar el paradigma: No sabemos cómo funciona, pero funciona.</p>
</section>
<section id="cómo-podemos-hacer-pensar-a-una-máquina" class="level1" data-number="9">
<h1 data-number="9"><span class="header-section-number">9</span> ¿Cómo podemos hacer pensar a una máquina?</h1>
<p>Para poder hacer pensar una máquina, o si queremos, para entrenarla y hacerla capaz de resolver un problema, necesitamos resolver un problema de minimización.</p>
<p>Los problemas de minimización son problemas matemáticos de carácter general que van mucho más allá de la inteligencia artificial. Para entender un problema de minimización, veamos un ejemplo.</p>
</section>
<section id="problemas-de-minimización-fundamentos-matemáticos" class="level1" data-number="10">
<h1 data-number="10"><span class="header-section-number">10</span> Problemas de minimización, fundamentos matemáticos</h1>
<p>Imaginemos que queremos establecer un modelo que relaciona el precio de la vivienda en una ciudad con alguna característica de la vivienda. Somos muy observadores y nos hemos dado cuenta de que a medida que una casa tiene más superficie, es más cara, así que construimos una gráfica que nos relaciona precio de la vivienda y superficie.</p>
<p>Viendo nuestra gráfica, nos damos cuenta también de que guardan una relación lineal. Olvidad por el momento las matemáticas, si os pido que me pintéis una línea que represente estos datos pintáis algo que va por aquí. En esto, los humanos somos muy buenos, determinamos cosas a ojo con gran precisión.</p>
<p>Si nos fijamos en la gráfica de la derecha, vemos que cuando colocamos la recta en su sitio llegamos a un mínimo de otra función. En ese caso, esa función corresponde a la suma cuadrática de todas las distancias de los puntos a la recta. Los detalles matemáticos una vez más nos dan igual. Lo importante es que podemos transformar un problema que es colocar una determinada línea, en otro que es minimizar una función de coste. Y para esto las máquinas son especialmente buenas, porque estos problemas de minimización se pueden programar por software tradicional.</p>
</section>
<section id="problemas-de-minimización-elementos-clave" class="level1" data-number="11">
<h1 data-number="11"><span class="header-section-number">11</span> Problemas de minimización, elementos clave</h1>
<p>Algunas reflexiones:</p>
<p>Todo esto funciona si la función de coste está bien definida, es decir, si realmente está elegida para guiar a la solución del problema.</p>
<p>El modelo resulta especialmente útil porque nos ayuda a reducir o incluso eliminar la influencia de variables ocultas. Llamamos variables ocultas a variables que introducen variaciones en nuestros datos, sin tener controlado su valor. Por ejemplo, sabemos que además de la superficie de un piso, su valor depende de su estado de conservación o de la altura respecto al suelo. No es lo mismo comprar un primero, que comprar un quinto. Eso puede explicar que en la gráfica, los puntos realmente no se ajusten sobre nuestra línea. Pero si encontramos que la variable que más determina el precio de la vivienda, es su superficie, nuestro modelo nos permite hacer esa predicción, independientemente de otras variables.</p>
<p>En general, los problemas reales dependen de muchas variables y esto conlleva dos problemas asociados: por un lado, no todas las variables sujetan con las misma intensidad el problemas y, por otra parte, no todas las variables pueden cambiar su valor de forma arbitraria porque algunas guardan relaciones con otras.</p>
</section>
<section id="varias-variables-con-diferentes-correlaciones" class="level1" data-number="12">
<h1 data-number="12"><span class="header-section-number">12</span> Varias variables con diferentes correlaciones</h1>
<p>En esas figuras mostramos gráficamente qué entendemos porque una variable sujete con más o menos intensidad a un problema.</p>
<p>Cuanto más pronunciado es el mínimo de la función de coste, mejor determinado está el parámetro, y a su vez, más cerca caen los puntos del modelo. Puntos muy dispersos indican mucho peor lo que queremos determinar.</p>
</section>
<section id="overfitting-sobreajuste" class="level1" data-number="13">
<h1 data-number="13"><span class="header-section-number">13</span> Overfitting (sobreajuste)</h1>
<p>Otro parámetro que es importante dentro de los problemas de minimización, y del entrenamiento de modelos de inteligencia artificial, es el concepto de overfitting o sobre ajuste.</p>
<p>Introduciendo un mayor número de parámetros en el modelo, podemos hacer que este se ajuste tanto como queramos a nuestros puntos medidos. Haciendo esto, el sistema pierde la capacidad de predecir esa verdad subyacente. Se identifica porque el modelo reproduce muy bien los puntos con los que se ha entrenado pero predice cosas absurdas cuando se le introducen puntos diferentes.</p>
</section>
<section id="ejemplos-de-minimización.-aplicación-a-la-radioterapia" class="level1" data-number="14">
<h1 data-number="14"><span class="header-section-number">14</span> Ejemplos de minimización. Aplicación a la radioterapia</h1>
<p>Entiendo que todos estos conceptos no resultan extraños porque los utilizamos continuamente nuestra práctica diaria.</p>
</section>
<section id="optimación-de-fluencias-para-tratamientos-vmat" class="level1" data-number="15">
<h1 data-number="15"><span class="header-section-number">15</span> Optimación de fluencias para tratamientos VMAT</h1>
<p>Imaginemos que queremos planificarlo un tratamiento de ORL para unas prescripciones dadas.</p>
</section>
<section id="section" class="level1" data-number="16">
<h1 data-number="16"><span class="header-section-number">16</span> </h1>
<p>Para hacer la planificación tenemos que resolver un problema de minimización. Definimos una función de coste introduciendo unos objetivos del plan, que guardan una cierta relación, más o menos difusa, con los objetivos de prescripción.</p>
<p>Hay muchas funciones de coste que producen una solución válida, entendiendo por solución válida aquella que cumple con los objetivos de prescripción. La función de coste suele ser muy dependiente de la persona que realiza la planificación. Es difícil saber si el problema está bien optimizado, es decir, si realmente hemos obtenido la mejor solución posible.</p>
</section>
<section id="correlaciones-entre-parámetros" class="level1" data-number="17">
<h1 data-number="17"><span class="header-section-number">17</span> Correlaciones entre parámetros</h1>
<p>En este tipo de problemas sufrimos continuamente el efecto de la correlación entre parámetros. Por ejemplo, como ocurre en este caso, si parte del tiroides está incluido dentro del PTV de dosis ganglionar e intento forzar La protección del tiroides, voy a afectar a la cobertura del PTV Ganglionar, de manera que, forzar la importancia de uno obliga a aumentar la importancia del otro si no quiero perder la cobertura del PTV.</p>
</section>
<section id="section-1" class="level1" data-number="18">
<h1 data-number="18"><span class="header-section-number">18</span> </h1>
<p>[Alternar las dos gráficas para hacerlo visible]</p>
</section>
<section id="cómo-funciona-una-red-neuronal-artificial" class="level1" data-number="19">
<h1 data-number="19"><span class="header-section-number">19</span> ¿Cómo funciona una red neuronal artificial?</h1>
</section>
<section id="section-2" class="level1" data-number="20">
<h1 data-number="20"><span class="header-section-number">20</span> </h1>
<p>Para entender el funcionamiento de una red neuronal podemos pensar en cómo trabaja una fábrica en cadena. Cada trabajador, en este caso cada capa de la red neuronal, realiza una función concreta. Coge el producto, los datos que queremos procesar, de la capa anterior, realiza su tarea y se lo pasa al siguiente.</p>
<p>Su estructura típica está formada por una capa de entrada que recibe la imagen o en general, los datos. Hay serie de capas ocultas que suelen realizar labores de convolución, activación y normalización, y una capa de salida que produce el resultado del modelo, algún tipo de predicción que el usuario ve como algo que depende de la entrada.</p>
<p>El número de capas ocultas y su tipo son una decisión de diseño, no está predifinido. Unas redes se diferencian de otras por qué capas tienen y cómo se conectan, no solo por cómo hayan sido entrenadas. Es decir, dos redes con el mismo diseño pueden producir modelos completamente diferentes dependiendo de los datos a los que han sido expuestas para entrenarlas, pero en general cada problema concreto requerirá un diseño de la red diferente.</p>
</section>
<section id="detalles-de-las-capas" class="level1" data-number="21">
<h1 data-number="21"><span class="header-section-number">21</span> Detalles de las capas</h1>
<p>Veamos algunas generalidades de qué hacen los tipos de capas más comununes</p>
<section id="capa-de-convolución-solo-para-redes-de-imagen" class="level2" data-number="21.1">
<h2 data-number="21.1" class="anchored" data-anchor-id="capa-de-convolución-solo-para-redes-de-imagen"><span class="header-section-number">21.1</span> Capa de convolución (solo para redes de imagen)</h2>
<p>Primero una definición: un ordenador representa una imagen como una colección de números dispuestos en el espacio,</p>
<p>Un filtro de convolución coloca una imagen más pequeña sobre la anterior, realiza una operación de acuerdo a sus valores y después se va desplazando sobre la imagen.</p>
<p>Lo importante que tenemos que recordar es que los ingenieros conocen determinados filtros que potencian los bordes o la textura.</p>
</section>
</section>
<section id="visualización-de-la-salida-de-algunos-filtros-de-convolución" class="level1" data-number="22">
<h1 data-number="22"><span class="header-section-number">22</span> Visualización de la salida de algunos filtros de convolución</h1>
<p>Aquí podéis ver cómo funcionan estos filtros.</p>
<p>En la parte de arriba veis el efecto de pasar un filtro de convolución que potencia las líneas horizontales y uno que potencia las verticales. Esto ayuda porque la máquina entenderá que los cuatros tienen los dos tipos de líneas y verá que los uno solo tienen el tipo vertical.</p>
<p>En el diseño del modelo se fija el número de filtros de convolución, pero los detalles de cada filtro, sus valores concretos, se establecen durante el entrenamiento, son parámetros ajustables del modelo.</p>
<p>En general la salida de los filtros de convolución pueden tener una interpretación no trivial para un ser humano</p>
</section>
<section id="función-de-activación" class="level1" data-number="23">
<h1 data-number="23"><span class="header-section-number">23</span> Función de activación</h1>
<p>La salida de las capas de convolución se suele mandar a capas de activación.</p>
<p>Emplean funciones que en nuestra jerga se llaman de rectificación: solo permiten el paso de determinados valores a partir de un valor de corte.</p>
<p>Imitan el disparo de una neurona: dependiendo del valor que le llega la neurona se activa o no.</p>
<p>Gráficamente después de pasar por el filtro de convolución la función de activación se queda con los valores digamos más destacados.</p>
</section>
<section id="pooling-reducción" class="level1" data-number="24">
<h1 data-number="24"><span class="header-section-number">24</span> Pooling (reducción)</h1>
<p>La siguiente capa se dedica al pooling o reducción.</p>
<p>Es parecida a la convolución pero es no lineal, de cada grupo de píxeles se queda con un valor, que puede ser por ejemplo el máximo.</p>
<p>Con esto se reduce el tamaño de las imágenes centrándose en las características más importantes.</p>
</section>
<section id="cómo-aprende-una-red-neuronal" class="level1" data-number="25">
<h1 data-number="25"><span class="header-section-number">25</span> ¿Cómo aprende una red neuronal?</h1>
</section>
<section id="entrenamiento-minimizar-un-error" class="level1" data-number="26">
<h1 data-number="26"><span class="header-section-number">26</span> Entrenamiento = Minimizar un error</h1>
<p>El entrenamiento se hace mediante un proceso de minimización. Y de minimización sabéis un montón: entrenar una red neuronal es como hacer IMRT: queremos encontrar la mejor combinación de pesos (parámetros) que cumplan unos objetivos y minimicen una función de coste.</p>
<p>Se pasa una imagen por la red y se realiza una primera predicción con parámetros ajustables probablemente en valores absurdos. La predicción se compara con el valor esperado correcto, es decir, necesitaremos una muestra de entrenamiento de calidad o curada. El resultado de la comparación se evalúa mediante una función de pérdida. Se ajustan los parámetros para reducir el error y el proceso se repite con millones de imágenes.</p>
</section>
<section id="tipos-de-aprendizaje" class="level1" data-number="27">
<h1 data-number="27"><span class="header-section-number">27</span> Tipos de aprendizaje</h1>
<p>Dependiendo de cómo tengamos clasificados los datos que empleamos en el entrenamiento se distinguen distintos tipos.</p>
<p>Llamamos aprendizaje supervisado cuando los datos que empleamos para el entrenamiento están previamente etiquetados.</p>
<p>Por ejemplo para enseñar a una IA a diagnosticar pulmonía a partir de radiografías de tórax podemos partir de una base de datos de radiografías que están etiquetas previamente por radiólogos o clasificadas. En este ejemplo tres casos: tórax normal, neumonía bacteriana o neumonía vírica.</p>
</section>
<section id="aprendizaje-no-supervisado" class="level1" data-number="28">
<h1 data-number="28"><span class="header-section-number">28</span> Aprendizaje No Supervisado</h1>
<p>En el aprendizaje no supervisado las etiquetas son generadas durante el entrenamiento por la propia IA, es decir, agrupa imágenes que presentan características comunes como que pertenecen a una clase dada. Por ejemplo puede llegar a reconocer la cara de George Clooney y ver que es diferente a la cara de otras personas. Evidentemente no es capaz de identificar a George Clooney en términos absolutos. Después del entrenamiento le tendremos que decir, la persona que has reconocido como, yo que sé, persona 1 es George Clooney. Fijaros que nosotros funcionamos de forma parecida. Podemos decir que conocemos a una persona porque es un vecino de tu barrio a pesar de que no sepamos su nombre o no hayamos hablando nunca con él.</p>
<p>En el ejemplo de la derecha la inteligencia artificial agrupo objetos como personas, estatuas, farolas,</p>
</section>
<section id="aprendizaje-auto-supervisado" class="level1" data-number="29">
<h1 data-number="29"><span class="header-section-number">29</span> Aprendizaje Auto-supervisado</h1>
<p>El aprendizaje auto-supervisado es un paso más allá en el que además las etiquetas son también generadas por la inteligencia artificial con el fin de obtener representaciones útiles.</p>
</section>
<section id="resumen-sobre-los-tipos-de-aprendizaje-en-ia" class="level1" data-number="30">
<h1 data-number="30"><span class="header-section-number">30</span> Resumen sobre los tipos de aprendizaje en IA</h1>
<p>Esta es una tabla que condensa lo que hemos visto sobre los distintos tipos de aprendizaje.</p>
</section>
<section id="ejemplo-entrenamiento-de-una-red-neuronal-de-reconocimiento-facial" class="level1" data-number="31">
<h1 data-number="31"><span class="header-section-number">31</span> Ejemplo: entrenamiento de una red neuronal de reconocimiento facial</h1>
<p>En mi caso tengo el problema de que no soy capaz de entender las cosas hasta que no trasteo con ellas.</p>
<p>Para comprender un poco mejor cómo funciona todo esto y meditar un poco sobre ello, he estado probando a configurar y entrenar una red de reconocimiento facial.</p>
<p>Las fotos de caras que he puesto hasta ahora forman parte de ese proyecto. Hay cuatro caras, los cuatro miembros de mi fammilia: mis dos hijos, mi mujer y yo.</p>
<p>En total he utilizado 150 fotos de caras que he preprocesado, se trata entonces de un proyecto de machine learning, con archivos de 150x150 píxeles en color.</p>
<p>Todo está en internet, el diseño de la red lo he fusilado. Tiene estas capas que se corresponden con lo que hemos visto en dos niveles: el primer nivel reduce la imagen a 126x126, la segunda a 30x30 pero tiene el doble de niveles para expresar sus características podéis pensar en términos de color.</p>
<p>Al final las últimas capas pasan reducen toda esa información a las cuatro posible salidas: Jose, Eva, Gema y César</p>
<p>La red necesita aproximadamente siete millones y medio de parámetros.</p>
<p>En un ordenador normalito, pero con GPU, tarda 11.63 segundos en realizar diez ciclos de minimización (no son muchos, pensad en lo que hacéis durante una planificación) y ya consigue una exactitud del modelo del 97%.</p>
<p>La exactitud se mide reservando una fracción de los datos de entrenamiento para comprobar el funcionamiento del modelo cuando ya se ha ajustado.</p>
</section>
<section id="optimización-por-retropropagación" class="level1" data-number="32">
<h1 data-number="32"><span class="header-section-number">32</span> Optimización por retropropagación</h1>
<p>El número de parámetros a ajustar es tan elevado que no es un problema sencillo de resolver. Necesitamos que el software de entrenamiento siga alguna estrategia que nos guie hacia una solución válida.</p>
<p>Lo que hace es que una vez que se ha calculado el error se propaga hacia atrás por la red buscando cuáles son los parámetros que tiene un mayor impacto en la reducción del error.</p>
<p>Si recordáis lo que explicamos al hablar de problemas de minimización buscaríamos los parámetros que más sujetan el modelo.</p>
</section>
<section id="modelos-de-inteligencia-artificial-clasificación-vs-regresión" class="level1" data-number="33">
<h1 data-number="33"><span class="header-section-number">33</span> Modelos de Inteligencia Artificial: Clasificación vs Regresión</h1>
</section>
<section id="naturaleza-del-problema" class="level1" data-number="34">
<h1 data-number="34"><span class="header-section-number">34</span> Naturaleza del problema</h1>
<p>Por su salida podemos distinguir dos tipos de modelos:</p>
<p>Los que producen un resultado discreto, determinados valores concretos o clases. Son adecuados para problemas de clasificación. Un ejemplo es el sistema de reconocimiento facial que hemos vista antes, cuatro salida concretas, el nombre de la persona a la que pertenece la foto.</p>
<p>En radioterapia estos tendrán utilidad poe ejemplo en la segmentación de imágenes, queremos saber si un pixel pertenece a una estructura que estamos contorneando o no.</p>
<p>Los que producen un resultado que se representa por un valor continuo. El ejemplo del modelo para predecir el precio de la vivienda estará dentro de esta clase.</p>
<p>En radioterapia un sistema para predecir una distribución de dosis es un ejemplo de este tipo de modelo.</p>
<p>Internamente, en los detalle de la red neuronal, los dos tipos de modelos son muy parecidos. Un problema de clasificación es como un problema de regresión con una capa o capas adicionales que pasan de la salida continua a la discreta.</p>
</section>
<section id="estructura-de-modelos-y-activaciones" class="level1" data-number="35">
<h1 data-number="35"><span class="header-section-number">35</span> Estructura de modelos y activaciones</h1>
<p>En esta tabla os resumo las características más importantes de los dos tipos de modelos</p>
</section>
<section id="aplicación-práctica-de-la-inteligencia-artificial-en-radioterapia" class="level1" data-number="36">
<h1 data-number="36"><span class="header-section-number">36</span> Aplicación práctica de la inteligencia artificial en radioterapia</h1>
<p>Ahora que ya tenemos una idea general de qué es la IA, vamos a entrar en lo que nos interesa de verdad: ¿para qué sirve en radioterapia? ¿Dónde se está usando ya y qué nos puede aportar en el futuro cercano?</p>
<p>Vamos a recorrer juntos estas aplicaciones. Veremos ejemplos, y lo más importante: hablaremos de qué pueden aportar a nuestro trabajo como técnicos y como físicos.</p>
<p>Antes, una aclaración muy importante: cuando hablamos de IA en radioterapia, no estamos hablando de sustituir a profesionales. Estamos hablando de herramientas que ayudan a hacer el trabajo mejor, más rápido, y con más consistencia.</p>
</section>
<section id="campos-de-aplicación" class="level1" data-number="37">
<h1 data-number="37"><span class="header-section-number">37</span> Campos de aplicación</h1>
<p>La IA en radioterapia actualmente encuentra aplicación en el registro deformable de imagen, en la generación de CT sintéticos, en la delimitación de estructuras y en la predicción de distribuciones de dosis bien para hacer una previsión del impacto dosimétrico sin necesidad de contar con una planificación detallada pensada para ser realizada en el acelerador, o bien para facilitar la planificación mediante algún tipo de automatización.</p>
<p>Yo no tengo una bola de cristal, no me atrevo a prever el futuro, pero si os fijáis todas estas aplicaciones están en mayor o menor medida relacionadas con la radioterapia adaptativa. Son dos campos que probablemente se desarrollen a la vez y no de forma independiente. Un desarrollo eficiente de flujos de trabajo necesarios para la radioterapia adaptativa no serán viables sin un apoyo en técnicas de inteligencia artificial.</p>
<p>He marcado en rojo el registro deformable porque por el momento es un pilar en el que se pueden sustentar los otros campos de aplicación como ahora veremos.</p>
</section>
<section id="registro-deformable-de-imagen" class="level1" data-number="38">
<h1 data-number="38"><span class="header-section-number">38</span> Registro deformable de imagen</h1>
<p>Recordemos lo que es registro de imagen: es el proceso de alinear dos o más imágenes obtenidas en diferentes momentos, desde diferentes dispositivos o con diferentes condiciones del paciente, de forma que los mismos puntos anatómicos coincidan en todas ellas.</p>
<p>Decimos que es deformable si si se permiten transformaciones locales no rígidas, como compresiones, expansiones y desplazamientos no uniformes. Hablando en plata, una de las imágenes la convertimos en un material moldeable y la estiramos, apretamos, deformamos para que coincida con la otra. El resultado del registro es la información de como conseguir ese solapamiento.</p>
<p>Insisto en todo lo que se deriva del registro deformable: es una pieza clave de la radioterapia adaptativa y sirve también para: fusionar imágenes multimodalidad, propagar estructuras contorneadas en una imagen a la otra, realizar acumulaciones de dosis recibidas en diferentes geometrías</p>
</section>
<section id="section-3" class="level1" data-number="39">
<h1 data-number="39"><span class="header-section-number">39</span> </h1>
<p>Para visualizar qué es un registro deformable, fijaros en esta imagen de simulación y en el llenado de la vejiga.</p>
</section>
<section id="section-4" class="level1" data-number="40">
<h1 data-number="40"><span class="header-section-number">40</span> </h1>
<p>En el momento del tratamiento el CBCT muestra este otro llenado</p>
</section>
<section id="section-5" class="level1" data-number="41">
<h1 data-number="41"><span class="header-section-number">41</span> </h1>
<p>El registro deformable me dice cuanto tengo que mover cada vovel para llevarlo a la situación de simulación.</p>
<p>Como digo, el resultado importante no es conseguir que las dos imágenes coincidan, una vez que coinciden las dos imágenes son iguales y no tenemos más información que con la imagen primera. Lo relevante es como se transforman porque esa transformación se puede aplicar a las estructuras en una para convertirlas en las estructuras que tendría la otra, o si pensamos que la dosis es recibida por tejidos, deformar la dosis al desplazar los tejidos y poder sumarla sobre una geometría única.</p>
</section>
<section id="métodos-de-realización-del-registro-deformable" class="level1" data-number="42">
<h1 data-number="42"><span class="header-section-number">42</span> Métodos de realización del registro deformable</h1>
<p>En esta tabla os resumo métodos para realizar el registro deformable de imágenes, y si habéis tenido una intución como creo, os felicito estáis en lo cierto, todos se basan en resolver un problema de minimización. Al final se basa en buscar el campo de deformación, o colección de desplazamientos que consiguen que una imagen se solape con la otra.</p>
<p>Las dificultades aparecen cuando introducimos imágenes de diferentes modalidades, es fácil intuir que para una máquina es más difícil saber cuando una resonancia queda bien solapada con un CT que si solapo dos CTs. Depende de la secuencia, lo mismo veo el hueso negro cuando en el CT es blanco. Muy fácil de interpretar para un humano pero para la máquina necesita algún refinamiento en la métrica de solapamiento. Por eso aparecen distintos métodos.</p>
<p>La otra aproximación es utilizar inteligencia artificial. Mediante los métodos convencionales se crea una base de registros validados de buena calidad y se entrena la red neuronal para que aprenda a buscar los campos de deformación.</p>
</section>
<section id="métodos-basados-en-aprendizaje-profundo-deep-learning-dir" class="level1" data-number="43">
<h1 data-number="43"><span class="header-section-number">43</span> Métodos basados en aprendizaje profundo (Deep Learning DIR)</h1>
<p>Para realizar registro deformable de imagen mediante deep learning se emplean redes neuronales con arquitecturas específicas diseñadas y optimizadas para esta aplicación.</p>
<p>Lo más interesante de esta forma de atacar el registro deformable mediante esta estrategia es que el gasto de tiempo se va en el entrenamiento de la red, pero una vez entrenada, a la red se le dan las dos imágenes y las registra de forma mucho más rápida de lo que puede hacer un procedimiento tradicional.</p>
<p>La velocidad es importante si estamos en flujos de trabajo de radioterapia adaptativa online, en los que queremos realizar una evaluación y modificación de un plan de tratamiento con el paciente en la máquina.</p>
<p>También son más flexibles en cuanto a las imágenes de entrada si el entrenamiento ha incluido mucha variedad de imágenes.</p>
<p>No todo pueden ser ventajas. Acordaros que estos sistemas son del tipo funcionan pero no sabemos cómo funcionan. Son siempre una caja negra que puede resultar difícil de validar clínicamente.</p>
<p>Mi visión personal sobre este asunto es que siempre tienen que ser supervisados por un humano. La red nos da un registro que nosotros evaluamos superponiendo las imágenes, no somos tan buenos como la máquina pero somos muy buenos evaluando si lo que ha hecho está bien o no.</p>
<p>Si habéis trabajado o jugado con ChatGPT entenderéis de lo que estoy hablando. Es una ayuda, una gran ayuda, siempre que sepas guiarle tú y decirle cuándo está alucinando. Si te fías a ciegas de lo primero que te diga, puedes acabar mal. Por ejemplo preparando esta presentación me ha indicado muchas referencias científicas que se ha inventado. ¿Qué pensarías de mí si os las pasara como bibliografía?</p>
<p>Y otra opinión personal. La responsabilidad de su uso es mía, porque se tienen que entender como herramientas que tú decides utilizar. No puedo decir: como lo ha hecho una inteligencia artificial que busquen al ingeniero que la ha entrenado para pedirle responsabilidades. Es como pedirle responsabilidades al fabricante de bolis por los cálculos que haces con un boli.</p>
</section>
<section id="comparativa-de-modelos-de-deep-learning-para-dir" class="level1" data-number="44">
<h1 data-number="44"><span class="header-section-number">44</span> Comparativa de modelos de Deep Learning para DIR</h1>
</section>
<section id="section-6" class="level1" data-number="45">
<h1 data-number="45"><span class="header-section-number">45</span> </h1>
<p>En estas dos diapositivas os dejo algunos detalles de los diferentes modelos que se están desarrollando.</p>
<p>No os quiero marear. Si os fijáis quizá lo más interesante es que VoxelMorph y SynthMorph no parten de registros previos, aprenden a registrar internamente.</p>
</section>
<section id="delimitación-de-volúmenes.-métodos-por-software-tradicional" class="level1" data-number="46">
<h1 data-number="46"><span class="header-section-number">46</span> Delimitación de volúmenes. Métodos por software tradicional</h1>
<p>Una de las tareas más demandantes en términos de tiempo consumido dentro de la radioterapia es la delimitación de estructuras.</p>
<p>Los métodos convencionales los conocéis todos:</p>
<ul>
<li>La segmentación por umbral se basa en quedarse con la parte de la imagen que se encuentra entre dos valores del número CT. Funciona bien en estrucuras con mucho contraste y siempre que no haya otras estructuras que contenga los mismmos valores. Aquí por ejemplo, nos sirve para para contornear el cráneo pero se nos cuelan los huesos de la nariz. Nos podemos apoyar en algún tipo de ROI o hacer correcciones manuales.</li>
</ul>
</section>
<section id="section-7" class="level1" data-number="47">
<h1 data-number="47"><span class="header-section-number">47</span> </h1>
<ul>
<li>Otro método tradicional es el crecimiento de regiones, ya sabéis, colocamos una semilla y la región crece hasta que llega a un borde, un salto abrupto en el número CT. La magnitud del salto es ajustable por el usuario.</li>
</ul>
<p>Una de las primeras aplicaciones que ha llegado de forma práctica a nuestros centros es la <strong>segmentación automática</strong>.</p>
</section>
<section id="section-8" class="level1" data-number="48">
<h1 data-number="48"><span class="header-section-number">48</span> </h1>
<ul>
<li>Y otro método tradicional son los llamados contornos activos o serpientes (snakes). Se pueden entender como una mejora de los contornos de crecimiento de regiones. En nuestro Hospital estamos dentro del mundo Varian. En ARIA estos algoritmos están dentro un apartado denominado Mago de la segmentación y están preprogramados para algunos órganos en concreto como la médula, los pulmones, el cerebro…</li>
</ul>
<p>Y como todos sabéis, aunque no lo he incluido en la presentación, el método tradicional por antonomasia de la delimitación de volúmenes es el contorneo manual basado en la experiencia y conocimiento del Técnico.</p>
</section>
<section id="delimitación-de-volúmenes-mediante-atlas" class="level1" data-number="49">
<h1 data-number="49"><span class="header-section-number">49</span> Delimitación de volúmenes mediante atlas</h1>
<p>A medio camino entre los métodos tradicionales y la inteligencia artificial están los métodos basados en Atlas. Hasta la irrupción de los sistemas de IA fueron la forma más prometedora de resolver el problema.</p>
<p>La idea es simple, se tiene una base de casos bien segmentados y con una amplia variabiliad. Cuando se va a segmentar un nuevo caso el sistema busca en su base cuales son los casos más parecidos al nuevo caso y realiza un <strong>registro deformable</strong> entre ambos. Realizado el registro transfiere las estructuras deformandolas con la información del registro.</p>
<p>Funciona en tanto el registro funcione bien. Cambios de posición del paciente, por diferente inmovilización, o cambios introducidos por cirugías, que matemáticamente suponen la introducción de discontinuidades, hacen que el sistema tienda a fallar.</p>
</section>
<section id="section-9" class="level1" data-number="50">
<h1 data-number="50"><span class="header-section-number">50</span> </h1>
<p>Os muestro la aplicación de Varian para segmentación por Atlas</p>
<p>En la parte de arriba se seleccionan las estructuras que se quieren contornear. Basándose en esto el sistema busca los casos más parecidos y a partir de ahí realiza el registro y crea las estructuras.</p>
<p>Si habéis trabajado con estos sistemas, son bastante dependientes de un flujo de trabajo estandarizado,</p>
</section>
<section id="delimitación-de-volúmenes-mediante-ia-basada-en-cnns" class="level1" data-number="51">
<h1 data-number="51"><span class="header-section-number">51</span> Delimitación de volúmenes mediante IA basada en CNNs</h1>
<p>Más allá de la radioterapia, lo que nosotros llamamos delimitación de estructuras, corresponde a un campo denominado segmentación de image, o proceso de dividir una imagen digital en regiones o segmentos significativos, agrupando los píxeles que comparten características similares (como color, intensidad o textura) para facilitar su análisis o interpretación. En medicina, se utiliza para identificar y delimitar automáticamente estructuras anatómicas o lesiones dentro de las imágenes.</p>
<p>Quiero decir con esto que es un campo de interés en Medicina en general, y que recibe mucha atención.</p>
<p>El problema de la segmentación es un problema de clasificación, se trata de asignar cada voxel a la estructura o estructuras a las que pertenece: por ejemplo un voxel de la vejiga puede ser clasificado simultáneamente como vejiga y PTV.</p>
</section>
<section id="arquitecturas-más-comunes" class="level1" data-number="52">
<h1 data-number="52"><span class="header-section-number">52</span> Arquitecturas más comunes</h1>
<p>Las arquitecturas más comunes para realizar segmentación son variaciones de un tipo de red llamada U-Net.</p>
<p>Si os acordáis de cuando hablamos de las arquitecturas de las redes neuronales decíamos que tendían a ir reduciendo las dimensiones de las imágenes. Por razones obvias esto no nos interesa en esta aplicación. Por eso mezclan en una parte de reducción de la resolución con otra en la que se mantiene la original, para no perder resolución, y eso es lo que les da forma de U.</p>
</section>
<section id="variantes-modernas-de-cnns-para-segmentación-médica" class="level1" data-number="53">
<h1 data-number="53"><span class="header-section-number">53</span> Variantes modernas de CNNs para segmentación médica</h1>
<p>Este un campo en pleno desarrollo actualmente. En este diapositiva os nombre algunos desarrollos. En la medicina actual, en radioterapia en particular, la imagen ha dejado de ser plana para pasar a ser 3D (CT, resonancia, PET) o 4D. Una parte de estos desarrollos se basan en generar redes que manejen de forma natural imágen 3D. Otros lo que hacen es que son capaces de identificar zonas en la imagen que requieran especial atención.</p>
</section>
<section id="entrenamiento-de-modelos-de-segmentación-con-cnns" class="level1" data-number="54">
<h1 data-number="54"><span class="header-section-number">54</span> Entrenamiento de modelos de segmentación con CNNs</h1>
<p>La forma de entrenamiento es la convencional, partir de una base de imágenes bien contorneadas de todo tipo de modalidades. Las funciones de pérdida que se emplean para este entrenamiento son similares a las que se utilizan en los métodos de registro deformable convencional.</p>
</section>
<section id="ejemplos-clínicos-reales-de-uso-de-cnns" class="level1" data-number="55">
<h1 data-number="55"><span class="header-section-number">55</span> Ejemplos clínicos reales de uso de CNNs</h1>
<p>Ya existen sistemas comerciales que utilizan estos metodologías</p>
<p>En esta tabla os hago un resumen de aplicaciones clínicas de estos métodos.</p>
<p>Se utilizan en segmentación automática de casos de próstata para radioterapia adaptativa en ResoLinac, o para segmentar determinaadas patologías como cabeza y cuello, mama, pulmón, estructuras cerebrales para radiocirgía.</p>
</section>
<section id="sistemas-comerciales-basados-en-cnn" class="level1" data-number="56">
<h1 data-number="56"><span class="header-section-number">56</span> Sistemas comerciales basados en CNN</h1>
<p>Y en esta otra tabla una lista de sistemas comerciales que utilizan redes neuronales convolucionales para la segmentación.</p>
<p>La lista no es exhaustiva. Seguro que en los stands del Congreso podéis ver muchos otros sistemas.</p>
<p>Los algoritmos de segmentación basados en IA pueden delinear órganos de riesgo y volúmenes diana con un nivel de precisión sorprendente. Esto no quiere decir que sustituyan el trabajo del técnico o del médico, sino que <strong>aceleran el proceso y mejoran la reproducibilidad</strong>.</p>
<p>Además, en muchos casos podemos editar fácilmente los contornos generados. Esto nos ahorra tiempo, especialmente en localizaciones anatómicas complejas.</p>
</section>
<section id="generación-de-imágenes-sintéticas" class="level1" data-number="57">
<h1 data-number="57"><span class="header-section-number">57</span> Generación de imágenes sintéticas</h1>
<p>Otro uso cada vez más extendido es la generación de <strong>CT sintéticos a partir de CBCT o de RM</strong>.</p>
<p>¿Por qué es esto útil? Pues porque el CBCT no tiene calidad suficiente para el cálculo de dosis, pero sí nos da información anatómica del día del tratamiento. Si conseguimos transformar ese CBCT en un CT ‘de verdad’ mediante IA, podemos <strong>hacer adaptación online</strong> con mucho más fundamento.</p>
<p>Lo mismo con la resonancia: si generamos un sCT a partir de una RM, podemos planificar sin necesidad de un escáner adicional.</p>
</section>
<section id="predicción-de-dosis-y-planificación-automática" class="level1" data-number="58">
<h1 data-number="58"><span class="header-section-number">58</span> Predicción de dosis y planificación automática</h1>
<p>Y más allá: la IA también se está usando para <strong>predecir distribuciones de dosis</strong> basadas en las estructuras del paciente. Es decir: sin hacer una optimización completa, el sistema ya nos da una idea de lo que se podría alcanzar, o nos propone una planificación inicial.</p>
<p>Esto abre la puerta a sistemas de planificación automática o guiada por IA, que no sustituyen al planificador, pero que sí pueden ahorrar tiempo y facilitar el trabajo.</p>
</section>
<section id="validación-y-fiabilidad" class="level1" data-number="59">
<h1 data-number="59"><span class="header-section-number">59</span> Validación y fiabilidad</h1>
<p>Todo esto suena muy bien… pero ¿podemos fiarnos?<br>
¿Hasta qué punto estos sistemas están validados?</p>
<p>Es una cuestión clave. En general, los sistemas basados en IA necesitan pasar por procesos de <strong>validación muy rigurosos</strong> antes de su uso clínico. Además, cada centro debe evaluar cómo se comportan en su entorno real.</p>
<p>Aquí el papel de los físicos y los técnicos es fundamental: debemos conocer bien las limitaciones de los algoritmos, detectar errores y saber cuándo no debemos confiar ciegamente.</p>
</section>
<section id="seguridad-y-responsabilidad" class="level1" data-number="60">
<h1 data-number="60"><span class="header-section-number">60</span> Seguridad y responsabilidad</h1>
<p>Y también hay que hablar de <strong>responsabilidad</strong>.</p>
<p>Aunque una IA haya segmentado un órgano, quien valida ese contorno es un profesional. Aunque un algoritmo sugiera una planificación, quien decide si es válida o no es el equipo clínico.</p>
<p>La IA es una herramienta. Potente, sí, pero una herramienta. La <strong>responsabilidad clínica sigue siendo humana</strong>.</p>
</section>
<section id="el-papel-del-técnico-y-del-físico" class="level1" data-number="61">
<h1 data-number="61"><span class="header-section-number">61</span> El papel del técnico y del físico</h1>
<p>Entonces, ¿qué cambia para nosotros?</p>
<p>Cambia que debemos <strong>entender cómo funciona la IA</strong>, saber cuándo confiar y cuándo revisar, aprender a usarla a nuestro favor.</p>
<p>Los técnicos tendrán que aprender a validar segmentaciones, a ajustar parámetros de planificación guiada, a usar interfaces nuevas.</p>
<p>Los físicos tendremos que incorporar estos sistemas a los flujos clínicos, validarlos, supervisarlos y formar a los demás.</p>
</section>
<section id="conclusión" class="level1" data-number="62">
<h1 data-number="62"><span class="header-section-number">62</span> Conclusión</h1>
<p>La IA no es una amenaza, es una oportunidad. Pero una oportunidad que exige conocimiento, criterio y formación.</p>
<p>Y sobre todo, trabajo en equipo. Porque si algo ha demostrado la radioterapia moderna es que solo funciona si trabajamos todos juntos: médicos, técnicos, físicos.</p>
<p>Gracias por vuestra atención.</p>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>