---
title: "Inteligencia artificial en radioterapia"
subtitle: Simposio de T√©cnicos. Congreso conjunto SEFM SEPR. Toledo 2025
format: 
  clean-revealjs:
    logo: images/2505-SEFM-SEPR_Toledo_400x122-400x122.png
    footer: "IA en Radioterapia"
  pptx: {}
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
author:
  - name: C√©sar Rodr√≠guez
    email: cesar.rodriguez@salud.madrid.org
    affiliations: Hospital Universitario de Fuenlabrada
css: styles.css
---


# ¬øQu√© es la inteligencia artificial?

---

## 1. Definici√≥n de IA {.smaller}

> La [**inteligencia artificial**]{.alert} (IA) es una rama de la inform√°tica que [**permite a las m√°quinas imitar funciones cognitivas**]{.alert} humanas como aprender, razonar, resolver problemas o tomar decisiones.

### Ejemplos cotidianos de IA

- Filtros de spam en el correo.
- Recomendaciones en Netflix o Spotify.
- Traductores autom√°ticos.
- Chatbots.
- Detecci√≥n de tumores en mamograf√≠as.

:::{.callout-note icon="true"}
## Cotidaneidad de la IA
Convivimos con la IA todos los d√≠as, incluso sin darnos cuenta. En medicina, puede ser una gran aliada.
:::

---

## 2. ¬øEn qu√© se diferencia de un software tradicional?

| Caracter√≠stica         | Software tradicional             | Inteligencia Artificial                |
|------------------------|----------------------------------|----------------------------------------|
| Instrucciones          | Programadas expl√≠citamente       | Aprende de datos                       |
| Capacidad de adaptaci√≥n| Fija                             | Se adapta con entrenamiento            |
| Ejemplo en radioterapia| C√°lculo de dosis con f√≥rmulas    | Segmentaci√≥n autom√°tica                |{.smaller}

> ‚úÖ **Analog√≠a culinaria**:  
> El software tradicional es como una receta;  
> la IA es como un cocinero que aprende probando.

---

## 3. Tipos principales de IA{.smaller}

### IA simb√≥lica

- Basada en reglas l√≥gicas.
- Utiliza sistemas expertos.
- Es la forma m√°s antigua de IA.

### Machine Learning (ML)

- Aprende patrones a partir de datos.
- Necesita entrenamiento con muchos ejemplos.
- Se ha convertido en el n√∫cleo de la IA moderna.

### Deep Learning (DL)

- Subconjunto de ML.
- Utiliza redes neuronales profundas.
- Muy eficaz en imagen m√©dica.

---

### Estructura de una red neuronal

![](images/NeuronalNetwork.png){fig-align="center"}


# Ejemplos

---
 
### Software tradicional. C√°lculo de la dosis

::: {.smaller-columns}
::: {.columns}
:::: {.column width="30%"}
![](images/CalculoDosis.png)
::::
:::: {.column width="70%"}
$$
\begin{aligned}
\colorbox{AquaMarine}{$\displaystyle\Omega_\gamma \cdot \nabla \Phi_\gamma(r, \Omega_\gamma, E_\gamma)$} = \\
\colorbox{Apricot}{$\displaystyle\rho_e(r) \int_0^\infty \int_{4\pi} \tilde{\sigma}_{C,\gamma}(E'_\gamma, E_\gamma, \Omega'_\gamma \cdot \Omega_\gamma) 
\Phi_\gamma(r, \Omega'_\gamma, E'_\gamma) d\Omega'_\gamma dE'_\gamma $}
\\
- \colorbox{SkyBlue}{$\displaystyle\rho_e(r) \sigma_{\text{totC},\gamma}(E_\gamma) \Phi_\gamma(r, \Omega_\gamma, E_\gamma)$}
\end{aligned}
$$

Ecuaci√≥n de Boltzmann de transporte de radiaci√≥n

:::: 
:::
:::

- Problemas complejos que se pueden reducir a algoritmos programables
- La velocidad de c√°lculo de la m√°quina supera completamente lo que es realizable por un humano

:::{.callout-tip icon="true"}
## Paradigma 
Sabemos c√≥mo funciona y **funciona** (*para lo que est√° programado*)
:::

---

### IA tradicional (√°rboles l√≥gicos). Reconocimiento de patrones

::: {.columns}
:::: {.column width="50%"}
![Reconocimiento de caracteres](images/handwrittendigits.png)
::::
:::: {.column width="50%"}
![Reconocimiento facial](images/CalculoDosis.png){height="150px"}
:::: 
:::

- La complejidad del problema crece tan r√°pidamente que son necesarias estructuras muy complicadas y muchos recursos.
- El humano supera con cierta facilidad los resultados de la m√°quina

:::{.callout-important icon="true"}
## Paradigma 
Sabemos c√≥mo funciona pero **no funciona** 
:::

# ¬øC√≥mo podemos hacer *pensar* a una m√°quina?

---

## Problemas de minimizaci√≥n
### Fundamentos matem√°ticos para el entrenamiento de sistemas

```{python}
import numpy as np
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Datos simulados: precio de vivienda (en miles de euros) vs superficie (m¬≤)
np.random.seed(123)
m_true = 3.  # precio por m¬≤ en miles de euros
b_true = 100.0  # precio base (m√≠nimo, en miles de euros)
x = np.linspace(30, 150, 20)  # superficies entre 30 m¬≤ y 150 m¬≤
y = m_true * x + b_true + np.random.normal(scale=20.0, size=len(x))  # precios con ruido

# Rango de valores de pendiente (precio por m¬≤)
m_values = np.linspace(m_true - 2, m_true + 2, 50)
J_values = [np.sum((y - (m * x + b_true))**2) for m in m_values]

# Crear figura con subgr√°ficos
fig = make_subplots(rows=1, cols=2, subplot_titles=["Ajuste lineal: Precio vs Superficie", "Funci√≥n de coste J(m)"])

# A√±adir trazas fijas (siempre visibles)
fig.add_trace(go.Scatter(x=x, y=y, mode='markers', name='Datos reales', marker=dict(color='black')), row=1, col=1)
fig.add_trace(go.Scatter(x=m_values, y=J_values, mode='lines', name='Funci√≥n de coste J(m)', line=dict(color='black')), row=1, col=2)

# Trazas din√°micas iniciales
m0 = m_values[0]
y_fit0 = m0 * x + b_true
J0 = J_values[0]

ajuste_inicial = go.Scatter(x=x, y=y_fit0, mode='lines', name='Ajuste lineal', line=dict(color='red'))
punto_J = go.Scatter(x=[m0], y=[J0], mode='markers', name='Valor de J', marker=dict(color='red', size=10))

fig.add_trace(ajuste_inicial, row=1, col=1)
fig.add_trace(punto_J, row=1, col=2)

# Frames para animar
frames = []

for i, m in enumerate(m_values):
    y_fit = m * x + b_true
    J = J_values[i]
    frame = go.Frame(
        data=[
            # Puntos originales (subplot 1,1)
            go.Scatter(x=x, y=y, mode='markers', marker=dict(color='black'),
                       xaxis='x1', yaxis='y1'),

            # L√≠nea de ajuste (subplot 1,1)
            go.Scatter(x=x, y=y_fit, mode='lines', line=dict(color='red'),
                       xaxis='x1', yaxis='y1'),

            # Curva de funci√≥n de coste J(m) (subplot 1,2)
            go.Scatter(x=m_values, y=J_values, mode='lines', line=dict(color='black'),
                       xaxis='x2', yaxis='y2'),

            # Punto rojo J(m) (subplot 1,2)
            go.Scatter(x=[m], y=[J], mode='markers', marker=dict(color='red', size=10),
                       xaxis='x2', yaxis='y2'),
        ],
        name=str(round(m, 2))
    )
    frames.append(frame)

# Slider
steps = [dict(method="animate",
              args=[[str(round(m, 2))],
                    dict(mode="immediate",
                         frame=dict(duration=0, redraw=True),
                         transition=dict(duration=0))],
              label=str(round(m, 2)))
         for m in m_values]

sliders = [dict(
    steps=steps,
    transition=dict(duration=0),
    x=0.1,
    y=0,
    currentvalue=dict(font=dict(size=16), prefix="Precio por m¬≤ = ", visible=True, xanchor='center'),
    len=0.8
)]

# Aplicar layout con solo el slider
fig.update_layout(
    sliders=sliders,
    height=500,
    width=1000
)

# Asignar los frames directamente
fig.frames = frames

# Etiquetas de ejes
fig.update_xaxes(title_text="Superficie (m¬≤)", row=1, col=1)
fig.update_yaxes(title_text="Precio (miles de euros)", row=1, col=1)
fig.update_xaxes(title_text="Precio por m¬≤ (miles de euros)", row=1, col=2)
fig.update_yaxes(title_text="J(m)", row=1, col=2)

fig.show()

```
---

### Elementos clave 

- El sistema establece el modelo minimizando una ***funci√≥n de coste*** que tiene que ser definida previamente

:::{.callout-tip icon="true"}
## C√°lculos de minimizaci√≥n 
La minimizaci√≥n es proceso programable por los procedimientos de software tradicional
:::

:::{.callout-important icon="true"}
## Importancia del modelo
El modelo nos permite **hacer predicciones** reduciendo la influencia de variables ocultas
:::

#### Dificultades con modelos que dependen de varias variables
- No todas las variables que podemos introducir en el modelo lo *sujetan* con la misma intensidad
- Algunas variables presentan correlaciones, es decir, la variaci√≥n de una implica la variaci√≥n de otra.


---

### Varias variables con diferentes correlaciones

```{python}

import numpy as np
import plotly.graph_objects as go
from plotly.subplots import make_subplots

np.random.seed(123)

# Simulaci√≥n 1: Precio vs Superficie (alta correlaci√≥n)
m1_true = 3.0  # mil ‚Ç¨/m¬≤
b1_true = 100.0
x1 = np.linspace(50, 150, 20)  # m¬≤
y1 = m1_true * x1 + b1_true + np.random.normal(scale=15.0, size=len(x1))

# Simulaci√≥n 2: Precio vs Altura (menor correlaci√≥n)
m2_true = 5.0  # mil ‚Ç¨/planta
b2_true = 200.0
x2 = np.linspace(1, 20, 20)  # plantas
y2 = m2_true * x2 + b2_true + np.random.normal(scale=40.0, size=len(x2))

# Ajustes
m1_fit, b1_fit = np.polyfit(x1, y1, 1)
y1_fit = m1_fit * x1 + b1_fit

m2_fit, b2_fit = np.polyfit(x2, y2, 1)
y2_fit = m2_fit * x2 + b2_fit

# Funciones de coste con rango de 5 unidades
m_values1 = np.linspace(m1_fit - 2.5, m1_fit + 2.5, 100)
J_values1 = [np.sum((y1 - (m * x1 + b1_true))**2) for m in m_values1]

m_values2 = np.linspace(m2_fit - 2.5, m2_fit + 2.5, 100)
J_values2 = [np.sum((y2 - (m * x2 + b2_true))**2) for m in m_values2]

# Crear figura con 4 subgr√°ficos
fig = make_subplots(rows=2, cols=2, subplot_titles=[
    "Precio vs Superficie",
    "Funci√≥n de coste J(m): Superficie",
    "Precio vs Altura",
    "Funci√≥n de coste J(m): Altura"
])

# Arriba izquierda: Ajuste superficie
fig.add_trace(go.Scatter(x=x1, y=y1, mode='markers', name='Datos superficie', marker=dict(color='black')), row=1, col=1)
fig.add_trace(go.Scatter(x=x1, y=y1_fit, mode='lines', name='Ajuste superficie', line=dict(color='red')), row=1, col=1)

# Arriba derecha: Coste superficie
fig.add_trace(go.Scatter(x=m_values1, y=J_values1, mode='lines', name='J(m): superficie', line=dict(color='black')), row=1, col=2)

# Abajo izquierda: Ajuste altura
fig.add_trace(go.Scatter(x=x2, y=y2, mode='markers', name='Datos altura', marker=dict(color='black')), row=2, col=1)
fig.add_trace(go.Scatter(x=x2, y=y2_fit, mode='lines', name='Ajuste altura', line=dict(color='red')), row=2, col=1)

# Abajo derecha: Coste altura
fig.add_trace(go.Scatter(x=m_values2, y=J_values2, mode='lines', name='J(m): altura', line=dict(color='black')), row=2, col=2)

# Etiquetas
fig.update_xaxes(title_text="Superficie (m¬≤)", row=1, col=1)
fig.update_yaxes(title_text="Precio (miles de ‚Ç¨)", row=1, col=1)
fig.update_xaxes(title_text="Pendiente m (mil ‚Ç¨/m¬≤)", row=1, col=2)
fig.update_yaxes(title_text="J(m)", row=1, col=2)

fig.update_xaxes(title_text="Altura (plantas)", row=2, col=1)
fig.update_yaxes(title_text="Precio (miles de ‚Ç¨)", row=2, col=1)
fig.update_xaxes(title_text="Pendiente m (mil ‚Ç¨/planta)", row=2, col=2)
fig.update_yaxes(title_text="J(m)", row=2, col=2)

fig.update_yaxes(range=[0, 1.e6], row=1, col=2)
fig.update_yaxes(range=[0, 1.e6], row=2, col=2)

fig.update_layout(
    height=650,
    width=800,
    showlegend=False
)

fig.show()


```

---

### **Overfitting** o sobreajuste

```{python}

import numpy as np
import plotly.graph_objects as go

# Datos originales: precio vs superficie
np.random.seed(123)
m_true = 3.0  # mil ‚Ç¨/m¬≤
b_true = 100.0
x = np.linspace(50, 150, 20)
y = m_true * x + b_true + np.random.normal(scale=15.0, size=len(x))

# Ajuste polin√≥mico de grado alto (overfitting)
degree = len(x)
coefs = np.polyfit(x, y, degree)
poly = np.poly1d(coefs)

# Evaluar el modelo en una malla m√°s fina
x_fine = np.linspace(40, 160, 500)
y_poly = poly(x_fine)

# Crear figura
fig = go.Figure()

# Datos
fig.add_trace(go.Scatter(x=x, y=y, mode='markers', name='Datos', marker=dict(color='black')))

# Ajuste polin√≥mico (overfitting)
fig.add_trace(go.Scatter(x=x_fine, y=y_poly, mode='lines', name='Modelo sobreajustado', line=dict(color='red', dash='dot')))

fig.update_yaxes(range=[200, 600])

# Layout
fig.update_layout(
    xaxis_title="Superficie (m¬≤)",
    yaxis_title="Precio (miles de ‚Ç¨)",
    width=800,
    height=500
)

fig.show()

```

- El modelo predice muy bien lo que ha aprendido
- Pierde la capacidad de predecir situaciones no entrenadas

# Ejemplos de minimizaci√≥n. Aplicaci√≥n a la radioterapia

## Optimaci√≥n de fluencias para tratamientos VMAT

![Tratamiento adyuvante de un hemicuello. Dosis de prescripci√≥n 66 Gy al primario y 56.1 Gy a las cadenas ganglionares](images/testORL.png)

---

##

![Definici√≥n del problema de optimizaci√≥n](images/DefinicionObjetivosOptimizacionVMAT.png)

> La **funci√≥n de coste no** es **√∫nica**.  
> Hay muchas funciones de coste que producen una soluci√≥n v√°lida;  
> se introduce variabilidad entre los planificadores.  
> Es dif√≠cil saber si la planificaci√≥n est√° realmente optimizada.  

---

## {.smaller}
#### Correlaciones entre par√°metros

![El tiroides en parte esta incluido en el PTV de cadenas](images/TiroidesRealista.PNG)

---

## {.smaller}
#### Correlaciones entre par√°metros

![Si forzamos el tiroides afectamos al PTV](images/TiroidesForzado.PNG)

# ¬øC√≥mo funciona una red neuronal artificial?

---

## {.smaller}

### Analog√≠a visual  
üß† **Red neuronal artificial = F√°brica en cadena**

Cada capa recibe una entrada, la procesa y pasa el resultado a la siguiente.

Cada neurona hace una operaci√≥n: pondera entradas, suma, aplica una funci√≥n.

### Estructura t√≠pica

::: {.columns}
:::: {.column width="50%"}
- **Capa de entrada**: recibe la imagen o los datos.  
- **Capas ocultas**: transforman progresivamente la informaci√≥n.  
  - Convoluci√≥n  
  - Activaci√≥n (triggers)  
  - Normalizaci√≥n / reducci√≥n  
- **Capa de salida**: genera una predicci√≥n.
::::
:::: {.column width="50%"}
![](images/FullyConnectedLayers.webp){fig-align="center" }
::::
:::

---

### Detalles de las capas

#### üî∏ Capa de convoluci√≥n *(solo en redes de imagen)*

- Aplica peque√±os filtros que se desplazan por la imagen.  

![](images/diagrama_convolucion.png)

- Cada filtro detecta un patr√≥n espec√≠fico: borde, textura, forma‚Ä¶  
- Resultado: un mapa de activaci√≥n que destaca ese patr√≥n.  

---

## {.smaller}

#### Visualizaci√≥n de la salida de algunos filtros de convoluci√≥n


::: {.columns}
:::: {.column width="50%"}
![](images/BordesHorizontales.webp){fig-align="right" height="100px"}
::::
:::: {.column width="50%"}
![](images/BordesVerticales.webp){fig-align="left" height="100px"}
::::
:::

::: {.columns}
:::: {.column width="60%"}
![](images/SalidaFiltrosConvolucionPrimeraCapa.png){fig-align="center"}
::::
:::: {.column width="40%"}
- En el dise√±o del modelo se fija el n√∫mero de filtros de convoluci√≥n
- Los detalles de cada filtro, sus valores concretos, se establecen durante el entrenamiento, son par√°metros ajustables del modelo
- En general la salida de los filtros de convoluci√≥n pueden tener una interpretaci√≥n no trivial para un humano
::::
:::

---

### üî∏ b) Funci√≥n de activaci√≥n (‚Äútrigger‚Äù)

- Imita el disparo neuronal: una funci√≥n no lineal decide si la neurona ‚Äúse activa‚Äù.  
- Ejemplo: funci√≥n ReLU (Rectified Linear Unit):  
  - Salida = 0 si entrada < 0  
  - Salida = entrada si entrada ‚â• 0  
- Esto permite que la red aprenda relaciones complejas, no lineales.

üìå **Visual sugerido**: gr√°fica de la funci√≥n ReLU.

---

### üî∏ c) Pooling (reducci√≥n)

- Reduce la dimensi√≥n de la informaci√≥n manteniendo lo importante.  
- Equivalente a hacer un ‚Äúzoom out‚Äù: pierde detalle, gana eficiencia.  
- En planificaci√≥n, ser√≠a como agrupar v√≥xeles en una regi√≥n de inter√©s.

---

## ¬øC√≥mo aprende una red neuronal?

---

### üîπ a) Entrenamiento = Minimizar un error

- Se pasa una imagen por la red ‚Üí predicci√≥n.  
- Se compara con la etiqueta real ‚Üí funci√≥n de p√©rdida.  
- Se ajustan los pesos de las conexiones para reducir el error.  
- Este proceso se repite con millones de im√°genes.  

üîß ‚ÄúEntrenar una red neuronal es como hacer IMRT: queremos encontrar la mejor combinaci√≥n de pesos (par√°metros) que cumplan unos objetivos y minimicen una funci√≥n de coste.‚Äù

---

### üîπ b) Funci√≥n de coste = ‚ÄúPlan de calidad‚Äù

- Ejemplo de funci√≥n de p√©rdida para segmentaci√≥n: penaliza si el contorno predicho no coincide con el real.  
- Cuanto peor la predicci√≥n, mayor el coste.  
- El algoritmo busca los pesos que minimicen ese coste.

üìå Comparaci√≥n √∫til:  
- En planificaci√≥n: DVH fuera de tolerancia = mal plan.  
- En IA: contorno incorrecto = alto coste ‚Üí se corrige.

---

### üîπ c) Optimizaci√≥n por retropropagaci√≥n

- Se calcula el error.  
- Se propaga hacia atr√°s a trav√©s de la red.  
- Se ajustan los pesos paso a paso con t√©cnicas como descenso de gradiente.

üìå **Mensaje final del bloque**:  
No es magia: es matem√°ticas + prueba y error + millones de datos.

